# -*- coding: utf-8 -*-
"""fork-of-titanic-attempt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hWevDrsPFVqm5PUGHXddFL4Y9ncOrdvf
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd 
import os
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import Perceptron
from sklearn.linear_model import SGDClassifier
from sklearn.ensemble import RandomForestClassifier

"""## Load Data
Create train and test datasets then combine for easier manipulation down the road
"""

#Load data
train = pd.read_csv('/kaggle/input/titanic/train.csv')
test = pd.read_csv('/kaggle/input/titanic/test.csv')

#Combine for easier manipulation
combine = [train, test]

"""## EDA"""

train.columns.values

"""**What can the first rows tell us about the data?**
* Categorical
    * Survived, Pclass, Name, Sex, Ticket, Cabin, Embarked
* Numerical
    * Continuous: Age, Fare
    * Discrete: SibSp, Parch
* Some tickets are alphanumeric
* Cabins are alphanumeric
* 3 different values for Pclass (1, 2, 3)
* Survived is 1 or 0
* Name may contain additional names surrounded by parentheses
"""

train.head(10)

"""**Which columns are missing values? What are the data types of each column?**

Age (some missing), Cabin (most missing), Embarked (only a few), and Fare (one)
"""

train.info()

test.info()

"""**Summary of Numerical Features**

* 38% of the sample survived (32% of the total population survived)
* Most people were in 2nd or 3rd class
* 75% of the sample were relatively young (age 38 or younger)
* The oldest person in the sample was 80 years old
* Less than 50% of passengers traveled with a sibling
* More than 75% of passengers traveled with parents or children
* Around 75% of the sample paid less than the mean ticket price of $32.20
    * A relatively small number of passengers must have paid for very expensive tickets (highest = $512)


"""

train.describe()

"""**Summary of Categorical Features**
* All names are unique
* Only two different sexes with the most common being male (577 males or 65% of total)
* Some duplicate tickets
* Some duplicate cabins
* 3 different cities of embarkation
"""

train.describe(include=['O'])

"""## Who was most likely to survive?
**Men or women?**
* 74% of women and only 19% of men
"""

#Women's and Men's survival rates (example code)
women = train.loc[train.Sex == 'female']["Survived"]
rate_women = sum(women)/len(women)

men = train.loc[train.Sex == 'male']['Survived']
rate_men = sum(men) / len(men)

print("% of women who survived:", rate_women)
print("% of men who survived:", rate_men)

"""**Rich or poor?**
* Over 90% of first and second class women compared to half of 3rd class women
* More first class than the combined amount of second and third class
"""

#Pivot table by gender and class
table = pd.pivot_table(train, index=['Sex', 'Pclass'],
                       values=['Age', 'Fare', 'Survived',])
table

#Table of Number of Siblings and percent Survived
train[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)

"""## Data Visualization
Survival by Class
"""

g = sns.FacetGrid(train, col='Survived')
g.map(plt.hist, 'Pclass', bins=3)

"""Survival by Age Group"""

g = sns.FacetGrid(train, col='Survived')
g.map(plt.hist, 'Age', bins=20)

"""## Feature Engineering"""

#Create Title
for df in combine:
    df['Title'] = df.Name.str.split('[,.]', expand=False).str[1]

pd.crosstab(train['Title'], train['Sex'])

#Clean up titles
rare = '|'.join(['Lady', 'the Countess','Capt', 'Col', 'Dona','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer'])

for df in combine:
    df['Title'] = df['Title'].str.replace(rare, 'Rare')
    df['Title'] = df['Title'].str.replace('Mlle', 'Miss')
    df['Title'] = df['Title'].str.replace('Ms', 'Miss')
    df['Title'] = df['Title'].str.replace('Mme', 'Mrs')

print(pd.crosstab(train['Title'], train['Sex']))
print()
print(train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())

"""### Convert categorical features to numerical"""

#Convert Sex and Title to numerical values
title_map = {' Mr': 1, ' Mrs': 2, ' Miss': 3, ' Master': 4, ' Rare': 5}
embarked_map = {'S': 1, 'C': 2, 'Q': 3}

for df in combine:
    df['Sex'] = df['Sex'].map({'female': 1, 'male': 0})
    df['Title'] = df['Title'].map(title_map).fillna(0)
    df['Embarked'] = df['Embarked'].map(embarked_map)

train.head(10)

#Drop less useful columns (Name, Ticket, and Cabin)
for df in combine:
    df.drop(columns=['Ticket', 'Cabin', 'Name'], inplace=True)
    #train = train.drop(columns=['Ticket', 'Cabin', 'Name'])
    #test = test.drop(columns=['Ticket', 'Cabin', 'Name'])

#Print unique Pclass to figure out where a problem started
train['Pclass'].unique()

"""### Fill in null values"""

#look for null values
train.info()

#Fill in missing age values by taking the mean of values grouped by Pclass and gender
median_ages = np.zeros((2, 3))    #array for median ages

for df in combine:
    for i in range(0, 2):         #male or female
        for j in range(0, 3):     #3 different Pclasses
            SP_df = df[((df['Sex'] == i) &
                         (df['Pclass'] == j+1))]['Age'].dropna()
            age_median = SP_df.median()
            median_ages[i, j] = age_median
    
    for i in range(0, 2):
        for j in range(0, 3):
            df.loc[ (df.Age.isnull()) & (df.Sex == i) & (df.Pclass == j +1), 'Age'] = median_ages[i, j]
    df['Age'] = df['Age'].astype(int)

train.head(10)

#Fill in Fare using the median of fare grouped by Pclass
median_fares = np.zeros((3,1))

for df in combine:
    for i in range (0, 3):
        fares = df[df['Pclass'] == i+1]['Fare']
        fares_median = fares.median()
        median_fares[i] = fares_median
    
    for i in range(0, 3):
        df.loc[ (df.Fare.isnull()) & df.Pclass == i + 1, 'Fare'] = median_fares[i]
    
    df['Fare'] = df['Fare'].astype(int)
    
test.info()

#Fill in embarked
embarked_mode = train.Embarked.dropna().mode()[0]

for df in combine:
    df['Embarked'] = df['Embarked'].fillna(embarked_mode)

"""## Simplify features by grouping"""

#Create age group based on ages
for df in combine:
    df.loc[df['Age'] <= 2, 'AgeGroup'] = 0
    df.loc[((df['Age'] > 2) & (df['Age'] <= 18)), 'AgeGroup'] = 1
    df.loc[((df['Age'] > 18) & (df['Age'] <= 29)), 'AgeGroup'] = 2
    df.loc[((df['Age'] > 29) & (df['Age'] <= 44)), 'AgeGroup'] = 3
    df.loc[((df['Age'] > 44) & (df['Age'] <= 65)), 'AgeGroup'] = 4
    df.loc[((df['Age'] > 65)), 'AgeGroup'] = 5
    
train.head(10)

#My first attempt changed all the values in the row to the median age
#The next couple of lines are my process for identifying and fixing the issue
#Verify problem values 
#**Problem solved! Leaving code to see process**
train['Pclass'].unique()

#Examine the rows containing the problematic values
expected = [1.0, 2.0, 3.0]

df[~df.Pclass.isin(expected)]

#Create FareBand feature
#First see where cut off between classes
train_agg = train.groupby('Pclass').agg({
    'Fare' : [np.min, np.max, np.mean, np.std] })

train_agg

#Create bands using a different method, as I realized the above is just Pclass
train['FareBands_'] = pd.qcut(train['Fare'], 6)
train[['FareBands_', 'Survived']].groupby(['FareBands_'], as_index=False).mean()\
    .sort_values(by='FareBands_', ascending=True)

train.head(10)

#Convert bands into ordinal values

for df in combine:
    df['Fare'] = df['Fare'].astype(float)
    df.loc[ df['Fare'] <= 7.775, 'FareBand'] = 0
    df.loc[(df['Fare'] > 7.775) & (df['Fare'] <= 8.662), 'FareBand'] = 1
    df.loc[(df['Fare'] > 8.662) & (df['Fare'] <= 14.454), 'FareBand'] = 2
    df.loc[(df['Fare'] > 14.454) & (df['Fare'] <= 26.0), 'FareBand'] = 3
    df.loc[(df['Fare'] > 26.0) & (df['Fare'] <= 52.369), 'FareBand'] = 4
    df.loc[ df['Fare'] > 52.369, 'FareBand'] = 5
    
#Drop farebands
train = train.drop(columns='FareBands_')

combine = [train, test]

"""## Create a new feature by combining existing features"""

for df in combine:
    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
    
train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)

"""## Create an artificial feature by combining two features"""

#Create artificial feature
for df in combine:
    df['AgeGroup*Class'] = df.AgeGroup * df.Pclass

"""## Feature Selection"""

#Feature Scaling of single column: Fare
#all_features = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Title', 'AgeGroup', 'FareBand', 'FamilySize', 'AgeGroup*Class']
features = ['Pclass', 'Sex', 'Title', 'FareBand', 'AgeGroup', 'FamilySize']

"""## Model, Predict and Solve"""

#Create training and test sets
X_train = train[features]
y_train = train['Survived']

X_test = test[features].copy()
X_train.shape, y_train.shape, X_test.shape

"""#### Logistic Regression"""

#Fit, train, and run model
logreg = LogisticRegression()
logreg.fit(X_train, y_train)

cv = cross_val_score(logreg, X_train, y_train, cv=5)
acc_logreg = cv.mean()
acc_logreg

#Print coefficients of each feature
coeff_df = pd.DataFrame(X_train.columns)
coeff_df.columns = ['Feature']
coeff_df["Correlation"] = pd.Series(logreg.coef_[0])

coeff_df.sort_values(by='Correlation', ascending=False)

"""#### Support Vector Classifier"""

svc = SVC()
svc.fit(X_train, y_train)

cv = cross_val_score(svc, X_train, y_train, cv=5)
acc_svc = cv.mean()
acc_svc

"""#### K-Nearest Neighbors"""

knn = KNeighborsClassifier(n_neighbors = 3)
knn.fit(X_train, y_train)

cv = cross_val_score(knn, X_train, y_train, cv=5)
print(cv)
acc_knn = cv.mean()
acc_knn

"""#### Random Forest Classifier"""

rfc = RandomForestClassifier(criterion = 'entropy',
                            max_depth = 20,
                            min_samples_leaf = 1,)
rfc.fit(X_train, y_train)

cv = cross_val_score(rfc, X_train, y_train, cv=5)
acc_rfc = cv.mean()
acc_rfc

"""#### Gradient Boosting Classifier"""

gbc = GradientBoostingClassifier(n_estimators=50,
                                 loss = 'exponential',
                                 learning_rate=0.5,
                                 max_depth=1,
                                 criterion = 'friedman_mse',
                                 random_state=0)
gbc.fit(X_train, y_train)

cv = cross_val_score(gbc, X_train, y_train, cv=5)
acc_gbc = cv.mean()
acc_gbc

"""#### Naive Bayes"""

gnb = GaussianNB()
gnb.fit(X_train, y_train)

cv = cross_val_score(gnb, X_train, y_train, cv=5)
acc_gnb = cv.mean()
acc_gnb

"""#### Perceptron"""

ptron = Perceptron()
ptron.fit(X_train, y_train)

cv = cross_val_score(ptron, X_train, y_train, cv=5)
acc_ptron = cv.mean()
acc_ptron

"""#### Hypertuning"""

#Tune hyperparameters
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import StratifiedShuffleSplit

sss = StratifiedShuffleSplit(n_splits=10, test_size = 0.3, random_state=0)

def grid_search_fun (classifier):
    params = [{'criterion': ['gini'], #Random Forest
               'max_depth': [20],
               #'max_features': ['auto', 3, 5, 8],
               #'class_weight': [None, 'balanced', 'balanced_subsample'],
               'max_leaf_nodes': [None],
               #'min_impurity_decrease': [0.0, 0.1, 0.50, 1],
               'min_samples_leaf': [1]
              }]
    
    grid_search = GridSearchCV(estimator = classifier,
                           param_grid = params,
                           scoring = 'recall',
                           cv = sss)
    grid_search.fit(X_train, y_train)
    best = grid_search.best_params_
    return str(best)

#print(grid_search_fun(rfc))

models = pd.DataFrame({
    'Model': ['Logistic Regression', 'Support Vector Classifier', 'KNN',
              'Random Forest', 'Gradient Boosting Classifier', 
              'Naive Bayes', 'Perceptron'],
    'Score': [acc_logreg, acc_svc, acc_knn,
              acc_rfc, acc_gbc, 
              acc_gnb, acc_ptron] })
models.sort_values(by='Score', ascending=False)

predictions = svc.predict(X_test)

output = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predictions})
output.to_csv('submission.csv', index=False)
print("Your submission was successfully saved!")
